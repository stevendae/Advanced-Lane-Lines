{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import collections\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "nx = 9\n",
    "ny = 6\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((ny*nx,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (nx,ny), corners, ret)\n",
    "        #write_name = 'corners_found'+str(idx)+'.jpg'\n",
    "        #cv2.imwrite(write_name, img)\n",
    "        cv2.imshow('img', img)\n",
    "        cv2.waitKey(500)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "img = cv2.imread('test_images/test0_0.jpg')\n",
    "img_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "def CameraCalibration(objpoints, imgpoints, img_size):\n",
    "\n",
    "    # Do camera calibration given object points and image points\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)\n",
    "    \n",
    "    return ret, mtx, dist, rvecs, tvecs\n",
    "\n",
    "    # Save the camera calibration result for later use (we won't worry about rvecs / tvecs)\n",
    "\n",
    "ret, mtx, dist, rvecs, tvecs = CameraCalibration(objpoints, imgpoints, img_size)    \n",
    "    \n",
    "dist_pickle = {}\n",
    "dist_pickle = {}\n",
    "dist_pickle[\"mtx\"] = mtx\n",
    "dist_pickle[\"dist\"] = dist\n",
    "pickle.dump(dist_pickle, open(\"camera_cal/camera_cal.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distortion Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DistortionCorrection(img, mtx, dist):\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color and Gradient Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Abs_Sobel_Thresh(image, channel = 'gray', orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Calculate directional gradient\n",
    "    # Apply threshold\n",
    "    if channel == 'gray':\n",
    "        img = Grayscale(image)\n",
    "    elif channel == 'sat':\n",
    "        img = S_Channel(image)\n",
    "        \n",
    "    if orient == 'x':\n",
    "        sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0)\n",
    "        abs_sobel = np.absolute(sobelx)\n",
    "    elif orient == 'y':\n",
    "        sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1)\n",
    "        abs_sobel = np.absolute(sobely)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    grad_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return grad_binary\n",
    "\n",
    "def Mag_Thresh_Binary(image, channel = 'gray', sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Calculate gradient magnitude\n",
    "    # Apply threshold\n",
    "    if channel == 'gray':\n",
    "        img = Grayscale(image)\n",
    "    elif channel == 'sat':\n",
    "        img = S_Channel(image)\n",
    "        \n",
    "    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_64F, 0 , 1, ksize=sobel_kernel)\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    scale_factor = np.max(gradmag)/255\n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8)\n",
    "    mag_binary = np.zeros_like(gradmag)\n",
    "    mag_binary[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "    return mag_binary\n",
    "\n",
    "def Dir_Threshold(image, channel = 'gray', sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Calculate gradient direction\n",
    "    # Apply threshold\n",
    "    if channel == 'gray':\n",
    "        img = Grayscale(image)\n",
    "        \n",
    "    elif channel == 'sat':\n",
    "        img = S_Channel(image)\n",
    "        \n",
    "    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    dirgrad = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    dir_binary = np.zeros_like(dirgrad)\n",
    "    dir_binary[(dirgrad >= thresh[0]) & (dirgrad <= thresh[1])] = 1\n",
    "    return dir_binary\n",
    "\n",
    "def S_Channel(image):\n",
    "    img = np.copy(image)\n",
    "    hsv = cv2. cvtColor(image, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    l_channel = hsv[:,:,1]\n",
    "    s_channel = hsv[:,:,2]\n",
    "    \n",
    "    return s_channel\n",
    "\n",
    "def Grayscale(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return gray\n",
    "# Choose a Sobel kernel size\n",
    "ksize = 3 # Choose a larger odd number to smooth gradient measurements\n",
    "\n",
    "def Combined_Grad(image, sat, ksize, thresh, mag_thresh, dir_binary):\n",
    "# Apply each of the thresholding functions\n",
    "    gradx = Abs_Sobel_Thresh(image, channel = 'sat', orient='x', sobel_kernel=ksize, thresh=(50, 175))\n",
    "    grady = Abs_Sobel_Thresh(image, channel = 'sat', orient='y', sobel_kernel=ksize, thresh=(50, 175))\n",
    "    mag_binary = Mag_Thresh_Binary(image, channel = 'sat', sobel_kernel=ksize, mag_thresh=(50,175))\n",
    "    dir_binary = Dir_Threshold(image, channel = 'sat', sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "\n",
    "    gradx_gray = Abs_Sobel_Thresh(image, channel = 'gray', orient='x', sobel_kernel=ksize, thresh=(50, 175))\n",
    "    grady_gray = Abs_Sobel_Thresh(image, channel = 'gray', orient='y', sobel_kernel=ksize, thresh=(50, 175))\n",
    "    mag_binary_gray = Mag_Thresh_Binary(image, channel = 'gray', sobel_kernel=ksize, mag_thresh=(50, 175))\n",
    "    dir_binary_gray = Dir_Threshold(image, channel = 'gray', sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "\n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1)) | ((gradx_gray == 1) & (grady_gray == 1)) | ((mag_binary_gray == 1) & (dir_binary_gray == 1))] = 1\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region of Interest Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Region_Of_Interest(image, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(image)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(image.shape) > 2:\n",
    "        channel_count = image.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(image, mask)\n",
    "    return masked_image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hough Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Hough_Trans(image, rho, theta, threshold, min_line_len, max_line_gap, center, line_image):\n",
    "    \n",
    "    image = np.uint8(255*image/np.max(image))\n",
    "    lines = cv2.HoughLinesP(image, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    l_lines_x = []\n",
    "    r_lines_x = []\n",
    "    l_lines_y = []\n",
    "    r_lines_y = []\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "            if (x1 < center) & (x2 < center):\n",
    "                l_lines_x.append(x1)\n",
    "                l_lines_y.append(y1)\n",
    "                l_lines_x.append(x2)\n",
    "                l_lines_y.append(y2)\n",
    "            elif (x1 > center) & (x2 > center):\n",
    "                r_lines_x.append(x1)\n",
    "                r_lines_y.append(y1)\n",
    "                r_lines_x.append(x2)\n",
    "                r_lines_y.append(y2)\n",
    "    if len(l_lines_x)>0:            \n",
    "        left_lane_line = np.polyfit(l_lines_x,l_lines_y,1) #left_lane_line = [m,b]\n",
    "    else: \n",
    "        left_lane_line = [0,0]\n",
    "    \n",
    "    if len(r_lines_x)>0:\n",
    "        right_lane_line = np.polyfit(r_lines_x,r_lines_y,1) #rightt_lane_line = [m,b]\n",
    "    else: \n",
    "        right_lane_line = [0,0]\n",
    "    \n",
    "    return left_lane_line, right_lane_line, line_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transform Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Transform_Var():\n",
    "    \n",
    "    src = np.float32([(190, 720), (548, 480), (740, 480), (1130, 720)])\n",
    "    dst = np.float32([(190, 720), (190, 0), (1130, 0), (1130, 720)])\n",
    "        \n",
    "    return src, dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Corners_Warp(img, src, dst, mtx, dist, img_size):\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    warped = cv2.warpPerspective(img, M, img_size)\n",
    "\n",
    "    return warped, M, Minv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Find_Lines(top_down):\n",
    "    histogram = np.sum(top_down[top_down.shape[0]//2:,:], axis=0)\n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((top_down, top_down, top_down))*255\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 7\n",
    "    # Set height of windows\n",
    "    window_height = np.int(top_down.shape[0]/nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = top_down.nonzero() # Return the indices of the elements that are non-zero.\n",
    "    nonzeroy = np.array(nonzero[0]) #array of y-coordinates of non zero elements of top_down\n",
    "    nonzerox = np.array(nonzero[1]) #corresponding array of x-coordinates of non zero elements of top_down\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        #Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = top_down.shape[0] - (window+1)*window_height \n",
    "        win_y_high = top_down.shape[0] - window*window_height #closer to bottom of frame\n",
    "        win_xleft_low = leftx_current - margin #left side of left box\n",
    "        win_xleft_high = leftx_current + margin #right side of left box\n",
    "        win_xright_low = rightx_current - margin #left side of right box\n",
    "        win_xright_high = rightx_current + margin\n",
    "        #Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img, (win_xleft_low, win_y_low),(win_xleft_high, win_y_high),(0,255,0),5)\n",
    "        cv2.rectangle(out_img, (win_xright_low, win_y_low),(win_xright_high, win_y_high),(0,255,0),5)\n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:\n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds]\n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    return left_fit, right_fit, leftx, lefty, rightx, righty, out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Line of Best Fit with Left and Right Lane Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linefit(top_down, left_fit, right_fit):\n",
    "    ploty = np.linspace(0, top_down.shape[0]-1, top_down.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    return ploty, left_fitx, right_fitx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipping Sliding Windows Step and Searching in Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextframe(top_down, left_fit, right_fit):\n",
    "    nonzero = top_down.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    margin = 100 #\n",
    "    left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "    right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] + margin)))  \n",
    "\n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    # Fit a second order polynomial to each\n",
    "    if len(leftx)>10:\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    else:\n",
    "        left_fit = [0,0,0]\n",
    "        \n",
    "    if len(rightx)>10:\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "    else:\n",
    "        right_fit = [0,0,0]\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, top_down.shape[0]-1, top_down.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    return left_fit, right_fit, ploty, left_fitx, right_fitx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Curvature(ploty, leftx, rightx):    \n",
    "    y_eval = np.max(ploty)\n",
    "\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    # Now our radius of curvature is in meters\n",
    "    #radius = (left_curverad+right_curverad)/2 \n",
    "    #print(left_curverad, 'm', right_curverad, 'm')\n",
    "    # Example values: 632.1 m    626.2 m\n",
    "    \n",
    "    return left_curverad, right_curverad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Offset(top_down):\n",
    "    \n",
    "    histogram = np.sum(top_down[top_down.shape[0]//2:,:], axis=0)\n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((top_down, top_down, top_down))*255\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "    midlane = (leftx_base+rightx_base)/2\n",
    "    midcar = (top_down.shape[1])/2\n",
    "    px_2_m = 0.004111\n",
    "    \n",
    "    offset = px_2_m*(np.absolute(midlane-midcar))\n",
    "    \n",
    "    return offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Drawing(image, top_down, left_fitx, right_fitx, ploty, Minv):\n",
    "# Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(top_down).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (image.shape[1], image.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    newwarp = newwarp.astype(dtype = np.uint8)\n",
    "    result = cv2.addWeighted(image, 1, newwarp, 0.3, 0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Line():\n",
    "    def __init__(self, n=5):\n",
    "        # number of previous frames to save line data from\n",
    "        self.n = n\n",
    "        # number of fits in buffer\n",
    "        self.n_buffered = 0\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = collections.deque([], maxlen=n)\n",
    "        # y values of the last n fits of the line\n",
    "        self.recent_yfitted = collections.deque([], maxlen=n)\n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float')                 \n",
    "\n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None\n",
    "        \n",
    "        #x values of the last n line pixels\n",
    "        self.recent_xplot = collections.deque([], maxlen=n)\n",
    "        #y values of the last n line pixels\n",
    "        self.recent_yplot = collections.deque([], maxlen=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Radius and Offset Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Caption(result, offset_dist, l_radius, r_radius):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    offset_str = str('offset from center: '+str(offset_dist*100)+'cm')\n",
    "    cv2.putText(result,offset_str,(20,30), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "    if l_radius and r_radius:\n",
    "        radius = round((l_radius/1000+r_radius/1000)/2,1) \n",
    "        str2 = str('radius of curvature: '+str(radius)+'km')\n",
    "        cv2.putText(result,str2,(20,60), font, 1,(255,255,255),2,cv2.LINE_AA)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging for Final Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Average(self):\n",
    "    fits = self.recent_xfitted\n",
    "    if len(fits)>0:\n",
    "        avg=0\n",
    "        for fit in fits:\n",
    "            avg +=np.array(fit)\n",
    "        avg = avg / len(fits)\n",
    "        \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Process Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Process_Image(image):\n",
    "    \n",
    "    img_size = (image.shape[1], image.shape[0])\n",
    "    \n",
    "    distort = DistortionCorrection(image, mtx, dist) #UNDISTORTED IMAGE\n",
    "    \n",
    "    #Binary Image Parameters\n",
    "    channel = 'sat'\n",
    "    ksize = 3\n",
    "    thresh = (50, 175)\n",
    "    mag_thresh = (50, 175)\n",
    "    dir_thresh = (0.7, 1.3)\n",
    "    \n",
    "    combined = Combined_Grad(distort, channel, ksize, thresh, mag_thresh, dir_thresh) #BINARY IMAGE\n",
    "    \n",
    "    src, dst = Transform_Var()\n",
    "    \n",
    "    #Hough Transform Parameters\n",
    "    rho = 1\n",
    "    theta = np.pi/180\n",
    "    threshold = 50\n",
    "    min_line_len = 30\n",
    "    max_line_gap = 40\n",
    "    line_image = np.copy(combined)*0\n",
    "    center = line_image.shape[1]/2\n",
    "    \n",
    "    if left.detected & right.detected: \n",
    "        \n",
    "        top_down, M, Minv = Corners_Warp(combined, src, dst, mtx, dist, img_size)\n",
    "        left_fit, right_fit, ploty, left_fitx, right_fitx = nextframe(top_down, left.current_fit, right.current_fit)\n",
    "        \n",
    "        if left_fit[0] == 0:\n",
    "            left_fit = left.current_fit\n",
    "            left_fitx = left.allx\n",
    "            left.detected = False\n",
    "        else:\n",
    "            left.detected = True\n",
    "            \n",
    "        if right_fit[0] == 0:\n",
    "            right_fit = right.current_fit\n",
    "            right_fitx = right.allx\n",
    "            right.detected = False\n",
    "        else:\n",
    "            right.detected = True\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        len_y = img_size[1]\n",
    "        len_x = img_size[0]\n",
    "        vertices = np.array([[(0,len_y), (len_x/2,425), (len_x,len_y)]], dtype=np.int32)\n",
    "        masked_image, mask = Region_Of_Interest(combined, vertices)\n",
    "        left_lane_line, right_lane_line, line_image = Hough_Trans(masked_image, rho, theta, threshold, min_line_len, max_line_gap, center, line_image)\n",
    "        \n",
    "        if (left_lane_line[0] != 0) & (right_lane_line[0] != 0):\n",
    "            \n",
    "            top_down, M, Minv = Corners_Warp(combined, src, dst, mtx, dist, img_size)\n",
    "            left_fit, right_fit, leftx, lefty, rightx, righty, out_img = Find_Lines(top_down)\n",
    "            ploty, left_fitx, right_fitx = linefit(top_down, left_fit, right_fit)\n",
    "            left.detected = True\n",
    "            right.detected = True\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            top_down, M, Minv = Corners_Warp(combined, src, dst, mtx, dist, img_size)\n",
    "            left_fitx = left.allx\n",
    "            right_fitx = right.allx\n",
    "            left_fit = left.current_fit\n",
    "            right_fit = right.current_fit\n",
    "            ploty = np.linspace(0, top_down.shape[0]-1, top_down.shape[0])\n",
    "            left.detected = False\n",
    "            right.detected = False\n",
    "            \n",
    "    left.diffs = np.absolute(left_fit - left.current_fit)\n",
    "    right.diffs = np.absolute(right_fit - right.current_fit)\n",
    "    \n",
    "    left.current_fit = left_fit\n",
    "    right.current_fit = right_fit\n",
    "    \n",
    "    left.recent_xfitted.append(left_fit)\n",
    "    right.recent_xfitted.append(right_fit)\n",
    "    \n",
    "    left.allx = left_fitx\n",
    "    right.allx = right_fitx\n",
    "    \n",
    "    left.ally = ploty\n",
    "    right.ally = ploty\n",
    "    \n",
    "    left.recent_xplot.append(left_fitx)\n",
    "    right.recent_xplot.append(right_fitx)\n",
    "    \n",
    "    left.recent_yplot.append(ploty)\n",
    "    right.recent_yplot.append(ploty)\n",
    "    \n",
    "    #Averaged Values of Fit, and Fitx\n",
    "    \n",
    "    if len(left.recent_xfitted)>0:\n",
    "        left_fit_avg = Average(left)\n",
    "    else:\n",
    "        left_fit_avg = left_fit\n",
    "    \n",
    "    if len(right.recent_xfitted)>0:\n",
    "        right_fit_avg = Average(right)\n",
    "    else:\n",
    "        right_fit_avg = right_fit\n",
    "        \n",
    "    #Plotted Fit Lines\n",
    "    \n",
    "    ploty, left_fitx_avg, right_fitx_avg = linefit(top_down, left_fit_avg, right_fit_avg)\n",
    "    \n",
    "    #Drawn Lines of Image\n",
    "    \n",
    "    result = Drawing(image, top_down, left_fitx_avg, right_fitx_avg, ploty, Minv)\n",
    "    \n",
    "    leftcurve, rightcurve = Curvature(ploty, left_fitx_avg, right_fitx_avg)\n",
    "    offset = Offset(top_down)\n",
    "    \n",
    "    Caption(result, offset, leftcurve, rightcurve)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Video():\n",
    "    white_output = 'test_videos_output/project_video.mp4'\n",
    "    ## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "    ## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "    ## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "    ## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "    ##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "    clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "    white_clip = clip1.fl_image(Process_Image) #NOTE: this function expects color images!!\n",
    "    %time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/challenge_video_output.mp4\n",
      "[MoviePy] Writing video test_videos_output/challenge_video_output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████▉| 1260/1261 [09:23<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/challenge_video_output.mp4 \n",
      "\n",
      "Wall time: 9min 23s\n"
     ]
    }
   ],
   "source": [
    "left = Line()\n",
    "right = Line()\n",
    "Video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
